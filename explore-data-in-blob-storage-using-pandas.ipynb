{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlockBlobService\n",
    "import pandas as pd\n",
    "import tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the variable with our specific values \n",
    "#storage account name will be the dns prefix we create \n",
    "#az login\n",
    "#az storage account list -o table\n",
    "#az storage account keys list -n YourAccount\n",
    "#to extract key \n",
    "#az storage account keys list -n YourAccount -o json --query \"[0].value\"\n",
    "#or else go to microsoft azure go to storage accounts select a name go to settings \n",
    "#and then copy the storage account name and key gi\n",
    "STORAGEACCOUNTNAME= <storage_account_name>\n",
    "STORAGEACCOUNTKEY= <storage_account_key>\n",
    "LOCALFILENAME= <local_file_name>\n",
    "CONTAINERNAME= <container_name>\n",
    "BLOBNAME= <blob_name>\n",
    "\n",
    "#download from blob\n",
    "t1=time.time()\n",
    "blob_service=BlockBlobService(account_name=STORAGEACCOUNTNAME,account_key=STORAGEACCOUNTKEY)\n",
    "blob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)\n",
    "t2=time.time()\n",
    "print((\"It takes %s seconds to download \"+BLOBNAME) % (t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data into a pandas dataframe from the downloadable file \n",
    "#localFile is the filepath \n",
    "dataframe_blobdata = pd.read_csv(LOCALFILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the number of rows and columns in dataset \n",
    "print('the size of the data is: %d rows and  %d columns' % dataframe_blobdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first/ last few rows in the dataset \n",
    "dataframe_blobdata.head(10)\n",
    "\n",
    "dataframe_blobdata.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the datatype of each column\n",
    "for col in dataframe_blobdata.columns:\n",
    "    print(dataframe_blobdata[col].name, ':\\t', dataframe_blobdata[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for basic stats \n",
    "dataframe_blobdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the number of enteries for each column\n",
    "dataframe_blobdata['<column_name>'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count missing values vs acc numbrt of entreies\n",
    "miss_num = dataframe_blobdata.shape[0] - dataframe_blobdata.count()\n",
    "print(miss_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the missing values :\n",
    "dataframe_blobdata_mode = dataframe_blobdata.fillna(\n",
    "    {'<column_name>': dataframe_blobdata['<column_name>'].mode()[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
